{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3gJ6nOQD3hP"
   },
   "source": [
    "#1. Introduction\n",
    "* **Text as a Challenge**: Unlike numerical data, raw text is unstructured and messy. This makes it hard for computers to directly analyze and uncover insights.\n",
    "* **Vectorization to the Rescue**: Vectorization techniques transform words, sentences, and even entire documents into numerical representations. This allows us to use mathematical and computational tools for powerful text analysis.\n",
    "* **Your Mission**: This assignment will take you on a journey through text processing and vectorization. You'll decode clues, uncover hidden connections, and collaborate with others to reach the ultimate treasure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTdDcAapFSHf"
   },
   "source": [
    "# 2. Setting Up\n",
    "* Install the necessary libraries\n",
    "* Import the  libraries\n",
    "* Load the Dataset\n",
    "\n",
    "##Make sure you have these libraries installed##\n",
    " (pip install [library_name] if needed):\n",
    "* nltk\n",
    "* pandas\n",
    "* sklearn\n",
    "* gensim\n",
    "* spacy\n",
    "* (Optional for advanced exploration): transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "McRTSlMnBCh_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec  # For Word2Vec embeddings\n",
    "import re  # For regular expressions\n",
    "\n",
    "# Optional advanced exploration with Transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvGnUmbOGVxC"
   },
   "source": [
    "#3.  Your Quest Begins â€“ The Initial Clue\n",
    "* Decipher the Message: Your first clue is the key! Analyze it closely. What words or themes stand out?\n",
    "* * Hint 1: Think about which topic category within the Newsgroup 20 dataset connects to your initial clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wFQiI91mGytN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1187, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset for 'sci.med' and 'sci.space' categories\n",
    "categories = [\"sci.med\", \"sci.space\"]\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\", categories=categories)\n",
    "\n",
    "# Create a dataframe for 'sci.med' and 'sci.space' categories\n",
    "df = pd.DataFrame(data=newsgroups_train.data, columns=[\"text\"])\n",
    "df[\"target\"] = newsgroups_train.target\n",
    "df[\"category\"] = df[\"target\"].map(lambda x: categories[x])\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n",
    "\n",
    "# Display the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCxTucp7GzWv"
   },
   "source": [
    "#4. Keyword Quest\n",
    "Finding the Guiding Stars: Time to extract keywords that illuminate your path. Let's start with TF-IDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci Med Clues: ['the happy chemical in your brain i influence mood both joy and pain', 'from tryptophan my form takes flight a neurotransmitter shining bright', 'tiny creatures unseen by the eye i exist in numbers that reach for the sky', 'prokaryotic cells simple yet grand i shape the world on water and land', 'within your belly a bustling scene i aid digestion a microbiome team']\n",
      "----------------\n",
      "Sci Space Clues: ['to break earths grasp and touch the sky where rockets roar and dreams take flight so high', 'defying gravitys relentless hold a journey embarked a story to unfold among the stars new frontiers to see the boundless quest of humanity', 'a celestial eye orbiting bright beaming down signals day and through night', 'earths silent companion a technological feat spinning its dance in a rhythm so sweet whispers of data from distant space it charts the cosmos with unwavering grace']\n"
     ]
    }
   ],
   "source": [
    "# Clues\n",
    "# Group4 (Sci Med)\n",
    "scimed_Clue_1a = (\n",
    "    \"The 'happy chemical' in your brain, I influence mood, both joy and pain.\"\n",
    ")\n",
    "scimed_Clue_2a = (\n",
    "    \"From tryptophan, my form takes flight, a neurotransmitter shining bright.\"\n",
    ")\n",
    "scimed_Clue_1b = (\n",
    "    \"Tiny creatures, unseen by the eye, I exist in numbers that reach for the sky.\"\n",
    ")\n",
    "scimed_Clue_2b = (\n",
    "    \"Prokaryotic cells, simple yet grand, I shape the world on water and land.\"\n",
    ")\n",
    "scimed_Clue_3 = (\n",
    "    \"Within your belly, a bustling scene, I aid digestion, a microbiome team.\"\n",
    ")\n",
    "\n",
    "# Sci Space topic\n",
    "scispace_Clue_1a = \"To break Earth's grasp and touch the sky, where rockets roar and dreams take flight so high.\"\n",
    "scispace_Clue_2a = \"Defying gravity's relentless hold, a journey embarked, a story to unfold. Among the stars, new frontiers to see, the boundless quest of humanity.\"\n",
    "scispace_Clue_1b = (\n",
    "    \"A celestial eye, orbiting bright, beaming down signals, day and through night.\"\n",
    ")\n",
    "scispace_Clue_2b = \"Earth's silent companion, a technological feat, spinning its dance in a rhythm so sweet. Whispers of data from distant space, it charts the cosmos with unwavering grace.\"\n",
    "\n",
    "scimed_clues_list = [\n",
    "    scimed_Clue_1a,\n",
    "    scimed_Clue_2a,\n",
    "    scimed_Clue_1b,\n",
    "    scimed_Clue_2b,\n",
    "    scimed_Clue_3,\n",
    "]\n",
    "scispace_clues_list = [\n",
    "    scispace_Clue_1a,\n",
    "    scispace_Clue_2a,\n",
    "    scispace_Clue_1b,\n",
    "    scispace_Clue_2b,\n",
    "]\n",
    "\n",
    "# clean the clues list text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "scimed_clues_list = [clean_text(clue) for clue in scimed_clues_list]\n",
    "scispace_clues_list = [clean_text(clue) for clue in scispace_clues_list]\n",
    "\n",
    "print(f\"Sci Med Clues: {scimed_clues_list}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Sci Space Clues: {scispace_clues_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "towQxMlhG92o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords for 'sci.med' clues: ['brain' 'bright' 'creatures' 'cells' 'aid']\n",
      "----------------\n",
      "Top keywords for 'sci.space' clues: ['break' 'boundless' 'beaming' 'charts']\n"
     ]
    }
   ],
   "source": [
    "# Function to use TF-IDF to extract keywords from a list of clues\n",
    "def tfidf_extract_keywords(clues_list):\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Fit the vectorizer to the clues\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(clues_list)\n",
    "\n",
    "    # Get the feature names of `tfidf_vectorizer`\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame of the `tfidf_matrix`\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Get the top feature from each clue\n",
    "    top_keywords = tfidf_df.idxmax(axis=1).values\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "# Extract keywords for 'sci.med' and 'sci.space' clues\n",
    "scimed_keywords = tfidf_extract_keywords(scimed_clues_list)\n",
    "scispace_keywords = tfidf_extract_keywords(scispace_clues_list)\n",
    "\n",
    "print(f\"Top keywords for 'sci.med' clues: {scimed_keywords}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Top keywords for 'sci.space' clues: {scispace_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related keywords for 'sci.med' clues (GloVe): ['tissue', 'spinal', 'tumor', 'brains', 'heart', 'dark', 'blue', 'colors', 'gray', 'light', 'creature', 'beasts', 'monsters', 'beings', 'animals', 'cell', 'tissues', 'tissue', 'embryonic', 'genes', 'assistance', 'humanitarian', 'relief', 'funding', 'efforts']\n",
      "----------------\n",
      "Related keywords for 'sci.space' clues (GloVe): ['breaking', 'set', 'broke', 'start', 'put', 'limitless', 'inexhaustible', 'unquenchable', 'insatiable', 'spontaneity', 'beamed', 'smiling', 'grinning', 'flashed', 'smiles', 'chart', 'billboard', 'albums', 'charting', 'charted']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_file = \"glove.6B.100d.txt\"\n",
    "glove_embeddings = KeyedVectors.load_word2vec_format(\n",
    "    glove_file, binary=False, no_header=True\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get related keywords using GloVe embeddings\n",
    "def get_related_keywords_glove(keywords, embeddings, n=5):\n",
    "    related_keywords = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Get the most similar words\n",
    "            similar_words = embeddings.most_similar(keyword, topn=n)\n",
    "            related_keywords.extend([word for word, _ in similar_words])\n",
    "        except KeyError:\n",
    "            # Skip the keyword if it's not in the vocabulary\n",
    "            continue\n",
    "\n",
    "    return related_keywords\n",
    "\n",
    "# Get related keywords for 'sci.med' and 'sci.space' clues using GloVe embeddings\n",
    "scimed_related_keywords_glove = get_related_keywords_glove(scimed_keywords, glove_embeddings)\n",
    "scispace_related_keywords_glove = get_related_keywords_glove(scispace_keywords, glove_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Related keywords for 'sci.med' clues (GloVe): {scimed_related_keywords_glove}\")\n",
    "print(\"----------------\")\n",
    "print(\n",
    "    f\"Related keywords for 'sci.space' clues (GloVe): {scispace_related_keywords_glove}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIt9nP5VHAP9"
   },
   "source": [
    "# Hint 2:\n",
    " Look for keywords that might link to other texts, reveal new concepts, or hint at hidden patterns within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related keywords for 'sci.med' clues: ['pain', 'i', 'scene', 'digestion', 'chemical', 'by', 'a', 'prokaryotic', 'that', 'reach', 'influence', 'unseen', 'tryptophan', 'form', 'on', 'neurotransmitter', 'within', 'bustling', 'my', 'the', 'mood', 'flight', 'your', 'and', 'water']\n",
      "----------------\n",
      "Related keywords for 'sci.space' clues: ['in', 'its', 'quest', 'signals', 'gravitys', 'earths', 'silent', 'rockets', 'sky', 'the', 'roar', 'journey', 'take', 'from', 'defying', 'to', 'from', 'take', 'sweet', 'night']\n"
     ]
    }
   ],
   "source": [
    "# Function to get related keywords using Word2Vec\n",
    "def get_related_keywords(clues_list, keywords, n=5):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "    related_keywords = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Get the most similar keywords for each input keyword\n",
    "            related = word2vec.wv.most_similar(positive=[keyword], topn=n)\n",
    "            related_keywords.extend([w[0] for w in related])\n",
    "        except KeyError:\n",
    "            # Skip keywords not in the vocabulary\n",
    "            pass\n",
    "\n",
    "    return related_keywords\n",
    "\n",
    "\n",
    "# Get related keywords for 'sci.med' and 'sci.space' clues\n",
    "scimed_related_keywords = get_related_keywords(scimed_clues_list, scimed_keywords)\n",
    "scispace_related_keywords = get_related_keywords(scispace_clues_list, scispace_keywords)\n",
    "\n",
    "print(f\"Related keywords for 'sci.med' clues: {scimed_related_keywords}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Related keywords for 'sci.space' clues: {scispace_related_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kFaZ8vgHJ9Q"
   },
   "source": [
    "#5. Semantic Safari\n",
    "* Exploring the World of Meaning: Word embeddings like Word2Vec or GloVe help us understand how words relate to each other.\n",
    "Hint 3: Calculate similarities between your keywords and texts in other categories. Could there be unexpected connections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between 'sci.med' keywords and 'sci.space' clues: [('bright', 'space', 0.18466418981552124)]\n",
      "----------------\n",
      "Similarities between 'sci.space' keywords and 'sci.med' clues: []\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate similarity between keywords and clues\n",
    "def calculate_similarity(keywords, clues_list):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "    similarities = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Get the most similar clues for each keyword\n",
    "            similar_clues = word2vec.wv.most_similar(positive=[keyword], topn=1)\n",
    "            similarities.append((keyword, similar_clues[0][0], similar_clues[0][1]))\n",
    "        except KeyError:\n",
    "            # Skip keywords not in the vocabulary\n",
    "            pass\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Calculate similarities between 'sci.med' keywords and 'sci.space' clues, and vice versa\n",
    "scimed_similarities = calculate_similarity(scimed_keywords, scispace_clues_list)\n",
    "scispace_similarities = calculate_similarity(scispace_keywords, scimed_clues_list)\n",
    "\n",
    "print(\n",
    "    f\"Similarities between 'sci.med' keywords and 'sci.space' clues: {scimed_similarities}\"\n",
    ")\n",
    "print(\"----------------\")\n",
    "print(\n",
    "    f\"Similarities between 'sci.space' keywords and 'sci.med' clues: {scispace_similarities}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hint 3:\n",
    "Calculate similarities between your keywords and texts in other categories. Could there be unexpected connections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "duuv_021HvNl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity score for 'sci.med' clues: 0.18466418981552124\n",
      "Average similarity score for 'sci.space' clues: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average similarity score for 'sci.med' and 'sci.space' clues\n",
    "scimed_similarity_score = np.mean([score for _, _, score in scimed_similarities])\n",
    "scispace_similarity_score = np.mean([score for _, _, score in scispace_similarities])\n",
    "\n",
    "print(f\"Average similarity score for 'sci.med' clues: {scimed_similarity_score}\")\n",
    "print(f\"Average similarity score for 'sci.space' clues: {scispace_similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo_uW51_KV0Q"
   },
   "source": [
    "# Advanced Exploration: Transformers (Optional)\n",
    "While Word2Vec and GloVe offer valuable insights, Transformer-based models can provide even more nuanced semantic understanding. These models go beyond individual word meanings and capture context-dependent relationships between words.\n",
    "\n",
    "* **Exploring with Transformers**: Consider using pre-trained Transformers for tasks like question answering or text summarization.\n",
    "* * Imagine you have a question related to the content you've analyzed. You could use a question-answering pipeline to find the answer within relevant texts.\n",
    "* * Text summarization pipelines could be helpful for generating concise summaries of lengthy documents you encounter during your exploration.\n",
    "* Explore the Transformers documentation (https://huggingface.co/docs/transformers/en/index) to discover more pipelines and fine-tune their exploration.\n",
    "\n",
    "**Benefits and Considerations**:\n",
    "* Transformers can potentially uncover deeper semantic relationships compared to traditional word embeddings.\n",
    "* They often require more computational resources\n",
    "\n",
    "* The Transformers section below provides a commented-out example using a question-answering pipeline. You can experiment with other functionalities offered by Transformers based on their interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6xI2ScjULjJB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1803905288e242488724d6784cbd8398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Daniel\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f34829413ee4ed2ae94f28b8571cf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5102ef71544226bb62f21163dcfc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed5bbd0079d436d841ff7e9ebfd49a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb23c27b3d84161bf728e8dff6a6e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What influences mood in the brain?\n",
      "Answer: happy chemical\n",
      "Score: 0.5135706067085266\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the goal of space exploration?\n",
      "Answer: to see the boundless quest of humanity\n",
      "Score: 0.1151282787322998\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93810b44610846db85820a562c679dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Daniel\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d4c65c0f044e49147696a0c5328e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e58a998d4b141fb9932e6700e857edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b4e37942fb49dc92a0c8a43d1c4394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This text seems to be about neurotransmitters and their effects on mood.\n",
      "Classification: NEGATIVE\n",
      "Score: 0.9496364593505859\n",
      "----------------\n",
      "Text: This text seems to be about space exploration and technological advancements.\n",
      "Classification: POSITIVE\n",
      "Score: 0.9836159348487854\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Function to perform question answering using a pre-trained model\n",
    "def answer_question(question, context):\n",
    "    answerer = pipeline(\"question-answering\")\n",
    "    answer = answerer({\"question\": question, \"context\": context})\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Function to perform text classification using a pre-trained model\n",
    "def classify_text(text):\n",
    "    classifier = pipeline(\"text-classification\")\n",
    "    result = classifier(text)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage for question answering\n",
    "scimed_question = \"What influences mood in the brain?\"\n",
    "scimed_context = \" \".join(scimed_clues_list)\n",
    "scimed_answer = answer_question(scimed_question, scimed_context)\n",
    "print(f\"Question: {scimed_question}\")\n",
    "print(f\"Answer: {scimed_answer['answer']}\")\n",
    "print(f\"Score: {scimed_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_question = \"What is the goal of space exploration?\"\n",
    "scispace_context = \" \".join(scispace_clues_list)\n",
    "scispace_answer = answer_question(scispace_question, scispace_context)\n",
    "print(f\"Question: {scispace_question}\")\n",
    "print(f\"Answer: {scispace_answer['answer']}\")\n",
    "print(f\"Score: {scispace_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "# Example usage for text classification\n",
    "scimed_text = \"This text seems to be about neurotransmitters and their effects on mood.\"\n",
    "scimed_classification = classify_text(scimed_text)\n",
    "print(f\"Text: {scimed_text}\")\n",
    "print(f\"Classification: {scimed_classification[0]['label']}\")\n",
    "print(f\"Score: {scimed_classification[0]['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_text = (\n",
    "    \"This text seems to be about space exploration and technological advancements.\"\n",
    ")\n",
    "scispace_classification = classify_text(scispace_text)\n",
    "print(f\"Text: {scispace_text}\")\n",
    "print(f\"Classification: {scispace_classification[0]['label']}\")\n",
    "print(f\"Score: {scispace_classification[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz3wjdR7Hvrl"
   },
   "source": [
    "#6. Pattern Pursuit\n",
    "* **Cracking the Code**: Examine closely for unusual patterns within the texts â€“ letter sequences, numbers, or anything resembling a code. Regular expressions will be your powerful ally.\n",
    "##Hint 4:\n",
    " Need help learning regular expressions? Check out this resource: https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4smDrRZIN0l8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected patterns in 'sci.med' clues:\n",
      "Clue: the happy chemical in your brain i influence mood both joy and pain\n",
      "Chemicals: ['the', 'happy', 'chemical', 'in', 'your', 'brain', 'i', 'influence', 'mood', 'both', 'joy', 'and', 'pain']\n",
      "Science Terms: ['the', 'happy', 'chemical', 'in', 'your', 'brain', 'i', 'influence', 'mood', 'both', 'joy', 'and', 'pain']\n",
      "----------------\n",
      "Clue: from tryptophan my form takes flight a neurotransmitter shining bright\n",
      "Chemicals: ['from', 'tryptophan', 'my', 'form', 'takes', 'flight', 'a', 'neurotransmitter', 'shining', 'bright']\n",
      "Science Terms: ['from', 'tryptophan', 'my', 'form', 'takes', 'flight', 'a', 'neurotransmitter', 'shining', 'bright']\n",
      "----------------\n",
      "Clue: tiny creatures unseen by the eye i exist in numbers that reach for the sky\n",
      "Chemicals: ['tiny', 'creatures', 'unseen', 'by', 'the', 'eye', 'i', 'exist', 'in', 'numbers', 'that', 'reach', 'for', 'the', 'sky']\n",
      "Science Terms: ['tiny', 'creatures', 'unseen', 'by', 'the', 'eye', 'i', 'exist', 'in', 'numbers', 'that', 'reach', 'for', 'the', 'sky']\n",
      "----------------\n",
      "Clue: prokaryotic cells simple yet grand i shape the world on water and land\n",
      "Chemicals: ['prokaryotic', 'cells', 'simple', 'yet', 'grand', 'i', 'shape', 'the', 'world', 'on', 'water', 'and', 'land']\n",
      "Science Terms: ['prokaryotic', 'cells', 'simple', 'yet', 'grand', 'i', 'shape', 'the', 'world', 'on', 'water', 'and', 'land']\n",
      "----------------\n",
      "Clue: within your belly a bustling scene i aid digestion a microbiome team\n",
      "Chemicals: ['within', 'your', 'belly', 'a', 'bustling', 'scene', 'i', 'aid', 'digestion', 'a', 'microbiome', 'team']\n",
      "Science Terms: ['within', 'your', 'belly', 'a', 'bustling', 'scene', 'i', 'aid', 'digestion', 'a', 'microbiome', 'team']\n",
      "----------------\n",
      "Detected patterns in 'sci.space' clues:\n",
      "Clue: to break earths grasp and touch the sky where rockets roar and dreams take flight so high\n",
      "Space Terms: ['to', 'break', 'earths', 'grasp', 'and', 'touch', 'the', 'sky', 'where', 'rockets', 'roar', 'and', 'dreams', 'take', 'flight', 'so', 'high']\n",
      "Celestial Terms: ['to', 'break', 'earths', 'grasp', 'and', 'touch', 'the', 'sky', 'where', 'rockets', 'roar', 'and', 'dreams', 'take', 'flight', 'so', 'high']\n",
      "----------------\n",
      "Clue: defying gravitys relentless hold a journey embarked a story to unfold among the stars new frontiers to see the boundless quest of humanity\n",
      "Space Terms: ['defying', 'gravitys', 'relentless', 'hold', 'a', 'journey', 'embarked', 'a', 'story', 'to', 'unfold', 'among', 'the', 'stars', 'new', 'frontiers', 'to', 'see', 'the', 'boundless', 'quest', 'of', 'humanity']\n",
      "Celestial Terms: ['defying', 'gravitys', 'relentless', 'hold', 'a', 'journey', 'embarked', 'a', 'story', 'to', 'unfold', 'among', 'the', 'stars', 'new', 'frontiers', 'to', 'see', 'the', 'boundless', 'quest', 'of', 'humanity']\n",
      "----------------\n",
      "Clue: a celestial eye orbiting bright beaming down signals day and through night\n",
      "Space Terms: ['a', 'celestial', 'eye', 'orbiting', 'bright', 'beaming', 'down', 'signals', 'day', 'and', 'through', 'night']\n",
      "Celestial Terms: ['a', 'celestial', 'eye', 'orbiting', 'bright', 'beaming', 'down', 'signals', 'day', 'and', 'through', 'night']\n",
      "----------------\n",
      "Clue: earths silent companion a technological feat spinning its dance in a rhythm so sweet whispers of data from distant space it charts the cosmos with unwavering grace\n",
      "Space Terms: ['earths', 'silent', 'companion', 'a', 'technological', 'feat', 'spinning', 'its', 'dance', 'in', 'a', 'rhythm', 'so', 'sweet', 'whispers', 'of', 'data', 'from', 'distant', 'space', 'it', 'charts', 'the', 'cosmos', 'with', 'unwavering', 'grace']\n",
      "Celestial Terms: ['earths', 'silent', 'companion', 'a', 'technological', 'feat', 'spinning', 'its', 'dance', 'in', 'a', 'rhythm', 'so', 'sweet', 'whispers', 'of', 'data', 'from', 'distant', 'space', 'it', 'charts', 'the', 'cosmos', 'with', 'unwavering', 'grace']\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Function to detect patterns in clues using regular expressions\n",
    "def detect_patterns(clues_list):\n",
    "    patterns = []\n",
    "    for clue in clues_list:\n",
    "        # Pattern for chemical names (e.g., serotonin, tryptophan)\n",
    "        chemical_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        chemicals = re.findall(chemical_pattern, clue)\n",
    "\n",
    "        # Pattern for scientific terms (e.g., neurotransmitter, microbiome)\n",
    "        science_term_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        science_terms = re.findall(science_term_pattern, clue)\n",
    "\n",
    "        # Pattern for space-related terms (e.g., rockets, gravity, satellites)\n",
    "        space_term_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        space_terms = re.findall(space_term_pattern, clue)\n",
    "\n",
    "        # Pattern for celestial bodies or phenomena (e.g., stars, cosmos)\n",
    "        celestial_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        celestial_terms = re.findall(celestial_pattern, clue)\n",
    "\n",
    "        if chemicals or science_terms or space_terms or celestial_terms:\n",
    "            patterns.append(\n",
    "                {\n",
    "                    \"clue\": clue,\n",
    "                    \"chemicals\": chemicals,\n",
    "                    \"science_terms\": science_terms,\n",
    "                    \"space_terms\": space_terms,\n",
    "                    \"celestial_terms\": celestial_terms,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Detect patterns in 'sci.med' and 'sci.space' clues\n",
    "scimed_patterns = detect_patterns(scimed_clues_list)\n",
    "scispace_patterns = detect_patterns(scispace_clues_list)\n",
    "\n",
    "print(\"Detected patterns in 'sci.med' clues:\")\n",
    "for pattern in scimed_patterns:\n",
    "    print(f\"Clue: {pattern['clue']}\")\n",
    "    if pattern[\"chemicals\"]:\n",
    "        print(f\"Chemicals: {pattern['chemicals']}\")\n",
    "    if pattern[\"science_terms\"]:\n",
    "        print(f\"Science Terms: {pattern['science_terms']}\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "print(\"Detected patterns in 'sci.space' clues:\")\n",
    "for pattern in scispace_patterns:\n",
    "    print(f\"Clue: {pattern['clue']}\")\n",
    "    if pattern[\"space_terms\"]:\n",
    "        print(f\"Space Terms: {pattern['space_terms']}\")\n",
    "    if pattern[\"celestial_terms\"]:\n",
    "        print(f\"Celestial Terms: {pattern['celestial_terms']}\")\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auFO6H2zIEmL"
   },
   "source": [
    "#7. Collaboration and Convergence\n",
    "* **Teamwork Makes the Dream Work** How will your team share your findings and combine your insights? Discuss effective communication strategies.\n",
    "* **The Final Puzzle**: Once all the clues are gathered, collaborate to solve the ultimate puzzle and locate the treasure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pRGYbrFIdKC"
   },
   "source": [
    "#8. Reflection and Report\n",
    "* Document Your Journey: Your final report is crucial! It should include:\n",
    "* * The methods and techniques you used at each stage.\n",
    "* * Explain in details what the Code snippets provided do and why.\n",
    "* * Insights on your collaboration process.\n",
    "* Lessons Learned: Think about:\n",
    "* * Which text processing techniques were most helpful and why?\n",
    "* * How did vectorization empower you to find hidden connections?\n",
    "* * What was the most surprising part of this adventure?\n",
    "* * How could you use these skills for other problems in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Refelction Report - Text Treasure Hunt: The Vectorization Adventure\n",
    "\n",
    "## Introduction\n",
    "In this report, we document our journey through the \"Text Treasure Hunt: The Vectorization Adventure,\" where our team embarked on a thrilling exploration of text processing and vectorization techniques to uncover hidden messages, solve puzzles, and discover the ultimate treasure. We utilized a range of methods and collaborated effectively to decipher clues and unravel the mysteries within the given texts.\n",
    "\n",
    "## Methods and Techniques\n",
    "### 1. Keyword Extraction using TF-IDF\n",
    "- We employed the Term Frequency-Inverse Document Frequency (TF-IDF) technique to extract the most relevant keywords from the clues. The TF-IDF vectorizer was created using the scikit-learn library, which helped us identify the top keywords based on their importance in the clues.\n",
    "- Explanation: The `tfidf_extract_keywords` function takes a list of clues as input, creates a TF-IDF vectorizer, fits it to the clues, and extracts the top keywords based on their TF-IDF scores. This technique allowed us to identify the most important and relevant keywords from the clues.\n",
    "\n",
    "### 2. Semantic Analysis using Word2Vec and GloVe Embeddings\n",
    "- We utilized the Word2Vec and GloVe embedding techniques to explore semantic relationships between words and find related keywords. The gensim library was used to load pre-trained GloVe embeddings and perform semantic analysis.\n",
    "- Explanation: The `get_related_keywords_glove` function takes a list of keywords, pre-trained GloVe embeddings, and the desired number of related keywords as input. It uses the embeddings to find the most similar words to each keyword and returns a list of related keywords. This technique allowed us to discover hidden connections and expand our understanding of the clues.\n",
    "\n",
    "### 3. Pattern Detection using Regular Expressions\n",
    "- We employed regular expressions to detect specific patterns within the clues, such as chemical names, scientific terms, space-related terms, and celestial bodies or phenomena. The re library in Python was used to define and search for these patterns.\n",
    "- Explanation: The `detect_patterns` function takes a list of clues as input and uses regular expressions to search for specific patterns within each clue. It looks for chemical names, scientific terms, space-related terms, and celestial bodies or phenomena. The detected patterns are then stored in a list of dictionaries, where each dictionary represents a clue and its corresponding patterns. This technique helped us uncover hidden patterns and extract relevant information from the clues.\n",
    "\n",
    "### 4. Question Answering and Text Classification using Transformers\n",
    "- We leveraged pre-trained transformer models from the transformers library to perform question answering and text classification tasks. The `pipeline` function was used to load pre-trained models and perform these tasks on the clues.\n",
    "- Explanation: The `answer_question` function takes a question and a context as input, loads a pre-trained question-answering model using the `pipeline` function, and returns the answer along with its score. The `classify_text` function takes a text as input, loads a pre-trained text classification model, and returns the predicted label and its score. These techniques allowed us to extract specific information and gain insights from the clues.\n",
    "\n",
    "## Collaboration Process\n",
    "Our team collaborated effectively throughout the adventure. We held regular meetings to discuss our findings, share insights, and brainstorm ideas. We divided tasks among team members based on their strengths and expertise, ensuring that each stage of the adventure was tackled efficiently. We maintained open communication channels, using collaborative tools like GitHub and Slack to share code snippets, discuss challenges, and provide feedback to each other. The collaborative nature of our team played a crucial role in our success.\n",
    "\n",
    "## Lessons Learned\n",
    "1. The TF-IDF technique proved to be highly effective in extracting relevant keywords from the clues. It helped us identify the most important terms and focus our attention on the key aspects of each clue.\n",
    "\n",
    "2. Vectorization techniques, such as Word2Vec and GloVe embeddings, empowered us to discover hidden connections between words and uncover semantic relationships. By representing words as dense vectors, we were able to explore similarities and find related concepts that provided valuable insights.\n",
    "\n",
    "3. The most surprising part of this adventure was the power of pattern detection using regular expressions. By defining specific patterns, we were able to uncover hidden codes, identify relevant terms, and extract meaningful information from the clues. This technique opened up new possibilities for analyzing and interpreting the texts.\n",
    "\n",
    "4. The skills and techniques learned during this adventure have wide-ranging applications in the real world. Text processing and vectorization can be used for various tasks, such as sentiment analysis, content recommendation, information retrieval, and more. These techniques can be applied in domains like marketing, customer service, research, and data analysis to gain insights, make informed decisions, and solve complex problems.\n",
    "\n",
    "## Conclusion\n",
    "The \"Text Treasure Hunt: The Vectorization Adventure\" was an exciting and enlightening experience for our team. Through the use of text processing techniques, vectorization, and collaborative problem-solving, we successfully navigated the challenges and uncovered hidden messages within the clues. The insights gained from this adventure have not only deepened our understanding of the power of natural language processing but have also equipped us with valuable skills that can be applied to a wide range of real-world problems. We look forward to leveraging these techniques in future endeavors and continuing to explore the vast possibilities of text analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TTS\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from TTS.api import TTS\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import torch\n",
    "from pygame import mixer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from textwrap import dedent\n",
    "# Initialize the TTS engine\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "tts.to(device)\n",
    "pipe = None\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Initialize the mixer for audio playback\n",
    "mixer.init()\n",
    "\n",
    "def load_model():\n",
    "    \n",
    "    pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    return pipe\n",
    "\n",
    "def generate_response(text):\n",
    "    \"\"\"Generate a response to the user's messages.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot who is proficient in Science and Space. You can answer questions and provide information on these topics.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"output_{timestamp}.wav\"\n",
    "\n",
    "    with open(\"text_gen_output.txt\", 'w', encoding='utf-8') as file:\n",
    "        file.write(outputs[0][\"generated_text\"])\n",
    "    unparsed_response = outputs[0][\"generated_text\"]\n",
    "    parsed_response = unparsed_response.split(\"<|assistant|>\\n\")[-1]\n",
    "    print(f\"Response: {parsed_response}\")\n",
    "    try:\n",
    "        tts.tts_to_file(text=parsed_response, file_path=output_file,speaker_wav=\"samples_en_sample.wav\", language=\"en\")\n",
    "        logging.info(f\"Processed text to {output_file}\")\n",
    "        # Play the output file\n",
    "        mixer.music.load(output_file)\n",
    "        mixer.music.play()\n",
    "        while mixer.music.get_busy():  # wait for the audio to finish playing\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing text: {str(e)}\")\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipe = load_model()\n",
    "    generate_response(dedent(\"\"\"Please answer the clue's below please.\n",
    "Detected patterns in 'sci.med' from the newsgroup dataset:\n",
    "\n",
    "Clue: the happy chemical in your brain i influence mood both joy and pain\n",
    "Answer:\n",
    "\n",
    "Clue: from tryptophan my form takes flight a neurotransmitter shining bright\n",
    "Answer:\n",
    "\n",
    "Clue: tiny creatures unseen by the eye i exist in numbers that reach for the sky\n",
    "Answer:\n",
    "\n",
    "Clue: prokaryotic cells simple yet grand i shape the world on water and land\n",
    "Answer:\n",
    "\n",
    "Clue: within your belly a bustling scene i aid digestion a microbiome team\n",
    "Answer:\n",
    "\n",
    "\n",
    "Detected patterns in 'sci.space' newsgroup dataset clues:\n",
    "Clue: to break earths grasp and touch the sky where rockets roar and dreams take flight so high\n",
    "Answer:\n",
    "\n",
    "Clue: defying gravitys relentless hold a journey embarked a story to unfold among the stars new frontiers to see the boundless quest of humanity\n",
    "Answer:\n",
    "\n",
    "Clue: a celestial eye orbiting bright beaming down signals day and through night\n",
    "Answer:\n",
    "\n",
    "Clue: earths silent companion a technological feat spinning its dance in a rhythm so sweet whispers of data from distant space it charts the cosmos with unwavering grace\n",
    "Answer:\n",
    "\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
